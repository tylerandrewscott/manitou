{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get started by importing spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../scratch/cleaned_comment_text.txt',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db04f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('../scratch/cleaned_comment_meta.txt',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a205fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f943eba5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop_words_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvaderSentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvaderSentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstop_words_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stop_words_list\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words_list'"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#from stop_words_list import stop_words_list\n",
    "#from wn_affect import wn_affect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f13421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - stop_words_list\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install stop_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "['\n",
    " '\n",
    " '/opt/anaconda3/lib/python3.8/lib-dynload', \n",
    " '/opt/anaconda3/lib/python3.8/site-packages', \n",
    " '/opt/anaconda3/lib/python3.8/site-packages/aeosa', \n",
    " '/opt/anaconda3/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2436939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f0a2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['Letter.Text.Clean'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48201d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = [sent_tokenize(d) for d in df2['Letter.Text.Clean'][:5000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62ed63f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentIntensityAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7b7c5846cca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# apply sentiment analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manalyser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentiment_score_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentiment_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentIntensityAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "# apply sentiment analysis\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_score_list = []\n",
    "sentiment_label_list = []\n",
    "\n",
    "for i in df['Letter.Text.Clean'].values.tolist():\n",
    "    sentiment_score = analyser.polarity_scores(i)\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Positive')\n",
    "    elif sentiment_score['compound'] > -0.05 and sentiment_score['compound'] < 0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Neutral')\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Negative')\n",
    "    \n",
    "df['sentiment'] = sentiment_label_list\n",
    "df['sentiment score'] = sentiment_score_list\n",
    "\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb141e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det food NOUN DET []\n",
      "food nsubj was AUX NOUN [The, had]\n",
      "we nsubj had AUX PRON []\n",
      "had relcl food NOUN AUX [we, yesterday]\n",
      "yesterday npadvmod had AUX NOUN []\n",
      "was ROOT was AUX AUX [food, delicious]\n",
      "delicious acomp was AUX ADJ []\n",
      "My poss time NOUN DET []\n",
      "time nsubj was AUX NOUN [My, in]\n",
      "in prep time NOUN ADP [Italy]\n",
      "Italy pobj in ADP PROPN []\n",
      "was ROOT was AUX AUX [time, enjoyable]\n",
      "very advmod enjoyable ADJ ADV []\n",
      "enjoyable acomp was AUX ADJ [very]\n",
      "I nsubj found VERB PRON []\n",
      "found ROOT found VERB VERB [I, be]\n",
      "the det meal NOUN DET []\n",
      "meal nsubj be AUX NOUN [the]\n",
      "to aux be AUX PART []\n",
      "be ccomp found VERB AUX [meal, to, tasty]\n",
      "tasty acomp be AUX ADJ []\n",
      "The det internet NOUN DET []\n",
      "internet nsubj was AUX NOUN [The]\n",
      "was ROOT was AUX AUX [internet, slow, .]\n",
      "slow acomp was AUX ADJ []\n",
      ". punct was AUX PUNCT []\n",
      "Our poss experience NOUN DET []\n",
      "experience nsubj was AUX NOUN [Our]\n",
      "was ROOT was AUX AUX [experience, suboptimal]\n",
      "suboptimal acomp was AUX ADJ []\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "      token.pos_,[child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5054979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      descriptive_term = token\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e8062a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "very enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d38008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious'}, {'aspect': 'time', 'description': 'very enjoyable'}, {'aspect': 'meal', 'description': 'tasty'}, {'aspect': 'internet', 'description': 'slow'}, {'aspect': 'experience', 'description': 'suboptimal'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  target = ''\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "      target = token.text\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  aspects.append({'aspect': target,\n",
    "    'description': descriptive_term})\n",
    "print(aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c668e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://textblob.readthedocs.io/en/dev/install.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be46f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious', 'sentiment': Sentiment(polarity=1.0, subjectivity=1.0)}, {'aspect': 'time', 'description': 'very enjoyable', 'sentiment': Sentiment(polarity=0.65, subjectivity=0.78)}, {'aspect': 'meal', 'description': 'tasty', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'internet', 'description': 'slow', 'sentiment': Sentiment(polarity=-0.30000000000000004, subjectivity=0.39999999999999997)}, {'aspect': 'experience', 'description': 'suboptimal', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "  aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6294f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### from the example. ### who to keep training the classifier\n",
    "#python -m textblob.download_corpora\n",
    "#from textblob.classifiers import NaiveBayesClassifier\n",
    "# We train the NaivesBayesClassifier\n",
    "#train = [\n",
    "#  ('Slow internet.', 'negative'),\n",
    "#  ('Delicious food', 'positive'),\n",
    "#  ('Suboptimal experience', 'negative'),\n",
    "#  ('Very enjoyable time', 'positive'),\n",
    "#  ('delicious food.', 'neg')\n",
    "#]\n",
    "#cl = NaiveBayesClassifier(train)\n",
    "# And then we try to classify some sample sentences.\n",
    "#blob = TextBlob(\"Delicious food. Very Slow internet. Suboptimal experience. Enjoyable food.\", classifier=cl)\n",
    "#for s in blob.sentences:\n",
    "#  print(s)\n",
    "#  print(s.classify())\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
